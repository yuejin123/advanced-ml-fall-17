{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced ML Part II // Lecture 04 Scratch // includes \"from mnist to svhn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# written by John P. Cunningham, for use in lecture\n",
    "# continues many of the conventions set out in Wenda Zhou's excellent tf tutorial\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as spio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_save(x, fname='foo.png', extent=None, show=True, cmap='gray'):\n",
    "    plt.imshow(x,cmap=cmap,extent=extent)\n",
    "    plt.axis('off')\n",
    "    plt.savefig('tmp/'+fname,bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate performance on some data \n",
    "def perf_eval(logit_pred, y_true):\n",
    "    \"\"\"a function to evaluate performance of predicted y values vs true class labels\"\"\"\n",
    "    # now look at some data\n",
    "    print('    sample pred: {0}\\n    sample true: {1}'.format(np.argmax(logit_pred[0:20],1),np.argmax(y_true[0:20],1)))\n",
    "    # avg accuracy\n",
    "    is_correct_vals = np.equal(np.argmax(logit_pred,1),np.argmax(y_true,1))\n",
    "    #accuracy_vals = np.mean(is_correct_vals)\n",
    "    #print('    mean classification accuracy: {0}%'.format(100*accuracy_vals))\n",
    "    # Dig in a little deeper.  Where did we make correct predictions?  Does this seem reasonable?\n",
    "    print('    correct predictions by class: {0}'.format(y_true[is_correct_vals,:].sum(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and sample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'X', 'y'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load SVHN data; get it yourself from http://ufldl.stanford.edu/housenumbers/\n",
    "mat_train = spio.loadmat('../data/svhn/train_32x32.mat')\n",
    "mat_test = spio.loadmat('../data/svhn/test_32x32.mat')\n",
    "mat_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_subtract=1\n",
    "n_val = 5000\n",
    "[inh,inw,inc,n] = mat_train['X'].shape\n",
    "#[_,_,_,n_test] = mat_test['X'].shape\n",
    "X_train = np.empty(shape=(inh,inw,inc,n-n_val), dtype=np.float32)\n",
    "X_val = np.empty(shape=(inh,inw,inc,n_val), dtype=np.float32)\n",
    "if mean_subtract==1:\n",
    "    for i in range(n-n_val):\n",
    "        for c in range(3):\n",
    "            X_train[:,:,c,i] = (mat_train['X'][:,:,c,i] - np.mean(mat_train['X'][:,:,c,i]))/255\n",
    "    for i in range(n_val):\n",
    "        for c in range(3):\n",
    "            X_val[:,:,c,i] = (mat_train['X'][:,:,c,i-n_val] - np.mean(mat_train['X'][:,:,c,i-n_val]))/255\n",
    "            \n",
    "else:\n",
    "    X_train = mat_train['X'][:,:,:,:n-n_val]\n",
    "    X_val = mat_train['X'][:,:,:,-n_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10]\n",
      "[10]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# unusual labeling scheme...\n",
    "print(np.unique(mat_train['y']))\n",
    "print(mat_train['y'][63900])\n",
    "# recode this in our usual way\n",
    "y_train0 = np.array([0 if y==10 else y[0] for y in mat_train['y'][:n-n_val]]).reshape([n-n_val,1])\n",
    "print(np.unique(y_train0))\n",
    "print(y_train0[63900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\n",
      "[ 1  2  3  4  5  6  7  8  9 10]\n",
      "[0]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[1 0 0 0 0 0 0 0 0 0]\n",
      "[ 4622 12931  9884  7897  6972  6379  5315  5218  4690  4349]\n"
     ]
    }
   ],
   "source": [
    "# now format y data correctly\n",
    "y_train = mat_train['y']\n",
    "# basic sanity check!  Uh oh...\n",
    "print(y_train[63900])\n",
    "print(np.unique(y_train))\n",
    "# fix this... note goofy bit about reshaping to (n, 1) to avoid (n,) shape issues\n",
    "y_train0 = np.array([0 if y==10 else y[0] for y in mat_train['y'][:n-n_val]]).reshape([n-n_val,1])\n",
    "y_vals = np.unique(y_train0).size\n",
    "# encode y as one hot vectors\n",
    "y_onehot = (np.arange(y_vals) == y_train0).astype(int)\n",
    "# y_onehot[63900,:] # check to make sure\n",
    "print(y_train0[63900])\n",
    "print(np.unique(y_train0))\n",
    "print(y_onehot[63900,:])\n",
    "print(np.sum(y_onehot,0))\n",
    "y_train = y_train0\n",
    "# and same for test\n",
    "y_val = np.array([0 if y==10 else y[0] for y in mat_train['y'][-n_val:]]).reshape([n_val,1])\n",
    "y_onehot_val = (np.arange(np.unique(y_val).size) == y_val).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_save(X_train[:,:,:,153], 'svhn_digit_0', cmap=None, show=False)\n",
    "plot_save(X_train[:,:,:,2], 'svhn_digit_1', cmap=None, show=False)\n",
    "plot_save(X_train[:,:,:,5000], 'svhn_digit_2', cmap=None, show=False)\n",
    "plot_save(X_train[:,:,:,341], 'svhn_digit_3', cmap=None, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the X_train tensor is: (32, 32, 3, 68257)\n",
      "The label data has the following unique labels: [0 1 2 3 4 5 6 7 8 9]\n",
      "Below is a sample data point (indexed by the LAST dimension of the tensor):\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x132f32f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its label is [9] and its onehot encoding is [0 0 0 0 0 0 0 0 0 1].\n",
      "The third index is the color channel (as you can see by permuting those indices)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x133534a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# one critical thing to realize is that tf interprets the shape of tensors to mean very specific things.\n",
    "# Being careless with this fact can cause substantial problems.\n",
    "# Let's understand the shape a bit.\n",
    "print('The shape of the X_train tensor is: {0}'.format(X_train.shape))\n",
    "print('The label data has the following unique labels: {0}'.format(np.unique(y_train)))\n",
    "print('Below is a sample data point (indexed by the LAST dimension of the tensor):')\n",
    "mm = 1\n",
    "plot_save(X_train[:,:,:,mm], 'svhn_c1')\n",
    "print('Its label is {0} and its onehot encoding is {1}.'.format(y_train[mm],y_onehot[mm]))\n",
    "print('The third index is the color channel (as you can see by permuting those indices)')\n",
    "plot_save(X_train[:,:,[0,2,1],mm], 'svhn_c2')  # for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQvMTYig0PyGGQvMTYij/AxTI46rrMA6pAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x132c95240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbksAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x133734ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this matters because we will take a batch of data, say the following samples:\n",
    "batch = [0,1,2]\n",
    "# and we will then reshape this data, as in:\n",
    "x_re = X_train[:,:,:,batch].reshape([np.shape(batch)[0],-1])\n",
    "# tf will then take this data one at a time from the first index.  \n",
    "xim = x_re[0,:]\n",
    "# let us reshape and plot that to make sure it is correct\n",
    "plot_save(xim.reshape([32,32,3]), 'svhn_c3')\n",
    "# ugh that is not right...\n",
    "# so we need to thoughtfully permute the indices of the tensor.  Get used to this and be careful.\n",
    "plot_save(X_train[:,:,:,batch].transpose([3,0,1,2]).reshape([np.shape(batch)[0],-1])[0,:].reshape([32,32,3]), 'svhn_c4')\n",
    "# better..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cnn conv stuff\n",
    "def conv(x, W):\n",
    "    \"\"\"simple wrapper for tf.nn.conv2d\"\"\"\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def maxpool(x):\n",
    "    \"\"\"simple wrapper for tf.nn.max_pool with stride size 2\"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def norm(x): \n",
    "    \"\"\"simple wrapper for tf.nn.lrn... See section 3.3 of Krizhevsky 2012 for details\"\"\"\n",
    "    return tf.nn.lrn(x, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# elaborate the compute_logits code to include a variety of models\n",
    "def compute_logits(x, model_type, pkeep):\n",
    "    \"\"\"Compute the logits of the model\"\"\"\n",
    "    if model_type=='lr':\n",
    "        W = tf.get_variable('W', shape=[32*32*3, 10])\n",
    "        b = tf.get_variable('b', shape=[10])\n",
    "        logits = tf.add(tf.matmul(x, W), b, name='logits_lr')\n",
    "    elif model_type=='cnn_cf':\n",
    "        # try a 1 layer cnn\n",
    "        n1 = 64\n",
    "        x_image = tf.reshape(x, [-1,32,32,3]) # batch, then width, height, channels\n",
    "        # cnn layer 1\n",
    "        W_conv1 = tf.get_variable('W_conv1', shape=[5, 5, 3, n1])\n",
    "        b_conv1 = tf.get_variable('b_conv1', shape=[n1])\n",
    "        h_conv1 = tf.nn.relu(tf.add(conv(x_image, W_conv1), b_conv1))\n",
    "        # fc layer to logits\n",
    "        h_conv1_flat = tf.reshape(h_conv1, [-1, 32*32*n1])\n",
    "        W_fc1 = tf.get_variable('W_fc1', shape=[32*32*n1, 10])\n",
    "        b_fc1 = tf.get_variable('b_fc1', shape=[10])\n",
    "        logits = tf.add(tf.matmul(h_conv1_flat, W_fc1), b_fc1, name='logits_cnn_cf')\n",
    "    elif model_type=='cnn_cnf':\n",
    "        # try a 1 layer cnn with a normalization layer\n",
    "        n1 = 64\n",
    "        x_image = tf.reshape(x, [-1,32,32,3]) # batch, then width, height, channels\n",
    "        # cnn layer 1\n",
    "        W_conv1 = tf.get_variable('W_conv1', shape=[5, 5, 3, n1])\n",
    "        b_conv1 = tf.get_variable('b_conv1', shape=[n1])\n",
    "        h_conv1 = tf.nn.relu(tf.add(conv(x_image, W_conv1), b_conv1))\n",
    "        # norm layer 1\n",
    "        h_norm1 = norm(h_conv1)\n",
    "        # fc layer to logits\n",
    "        h_flat = tf.reshape(h_norm1, [-1, 32*32*n1])\n",
    "        W_fc1 = tf.get_variable('W_fc1', shape=[32*32*n1, 10])\n",
    "        b_fc1 = tf.get_variable('b_fc1', shape=[10])\n",
    "        logits = tf.add(tf.matmul(h_flat, W_fc1), b_fc1, name='logits_cnn_cnf')     \n",
    "    elif model_type=='cnn_cpncpnff':\n",
    "        # 2 layer cnn\n",
    "        n1 = 32\n",
    "        n2 = 64\n",
    "        n3 = 1024\n",
    "        x_image = tf.reshape(x, [-1,32,32,3]) # batch, then width, height, channels\n",
    "        # cnn layer 1\n",
    "        W_conv1 = tf.get_variable('W_conv1', shape=[5, 5, 3, n1])\n",
    "        b_conv1 = tf.get_variable('b_conv1', shape=[n1])\n",
    "        h_conv1 = tf.nn.relu(tf.add(conv(x_image, W_conv1), b_conv1))\n",
    "        # pool 1\n",
    "        h_pool1 = maxpool(h_conv1)\n",
    "        # norm 1\n",
    "        h_norm1 = norm(h_pool1)\n",
    "        # cnn layer 2\n",
    "        W_conv2 = tf.get_variable('W_conv2', shape=[5, 5, n1, n2])\n",
    "        b_conv2 = tf.get_variable('b_conv2', shape=[n2])\n",
    "        h_conv2 = tf.nn.relu(tf.add(conv(h_norm1, W_conv2), b_conv2))\n",
    "        # pool 2\n",
    "        h_pool2 = maxpool(h_conv2)\n",
    "        # norm 2\n",
    "        h_norm2 = norm(h_pool2)\n",
    "        # fc layer to logits (8x8 since 2 rounds of maxpool)\n",
    "        h_norm2_flat = tf.reshape(h_norm2, [-1, 8*8*n2])\n",
    "        W_fc1 = tf.get_variable('W_fc1', shape=[8*8*n2, n3])\n",
    "        b_fc1 = tf.get_variable('b_fc1', shape=[n3])\n",
    "        h_fc1 = tf.nn.relu(tf.add(tf.matmul(h_norm2_flat, W_fc1), b_fc1))\n",
    "        # one more fc layer\n",
    "        # ... again, this is the logistic layer with softmax readout\n",
    "        W_fc2 = tf.get_variable('W_fc2', shape=[n3,10])\n",
    "        b_fc2 = tf.get_variable('b_fc2', shape=[10])\n",
    "        logits = tf.add(tf.matmul(h_fc1, W_fc2), b_fc2, name='logits_cnn_cpncpnff')\n",
    "    elif model_type=='cnn_cpncpnfdf':\n",
    "        # same as above but add dropout.\n",
    "        # 2 layer cnn\n",
    "        n1 = 32\n",
    "        n2 = 64\n",
    "        n3 = 1024\n",
    "        x_image = tf.reshape(x, [-1,32,32,3]) # batch, then width, height, channels\n",
    "        # cnn layer 1\n",
    "        W_conv1 = tf.get_variable('W_conv1', shape=[5, 5, 3, n1])\n",
    "        b_conv1 = tf.get_variable('b_conv1', shape=[n1])\n",
    "        h_conv1 = tf.nn.relu(tf.add(conv(x_image, W_conv1), b_conv1))\n",
    "        # pool 1\n",
    "        h_pool1 = maxpool(h_conv1)\n",
    "        # norm 1\n",
    "        h_norm1 = norm(h_pool1)\n",
    "        # cnn layer 2\n",
    "        W_conv2 = tf.get_variable('W_conv2', shape=[5, 5, n1, n2])\n",
    "        b_conv2 = tf.get_variable('b_conv2', shape=[n2])\n",
    "        h_conv2 = tf.nn.relu(tf.add(conv(h_norm1, W_conv2), b_conv2))\n",
    "        # pool 2\n",
    "        h_pool2 = maxpool(h_conv2)\n",
    "        # norm 2\n",
    "        h_norm2 = norm(h_pool2)\n",
    "        # fc layer to logits (8x8 since 2 rounds of maxpool)\n",
    "        h_norm2_flat = tf.reshape(h_norm2, [-1, 8*8*n2])\n",
    "        W_fc1 = tf.get_variable('W_fc1', shape=[8*8*n2, n3])\n",
    "        b_fc1 = tf.get_variable('b_fc1', shape=[n3])\n",
    "        h_fc1 = tf.nn.relu(tf.add(tf.matmul(h_norm2_flat, W_fc1), b_fc1))\n",
    "        # insert a dropout layer here.\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, pkeep)\n",
    "        # one more fc layer\n",
    "        # ... again, this is the logistic layer with softmax readout\n",
    "        W_fc2 = tf.get_variable('W_fc2', shape=[n3,10])\n",
    "        b_fc2 = tf.get_variable('b_fc2', shape=[10])\n",
    "        logits = tf.add(tf.matmul(h_fc1_drop, W_fc2), b_fc2, name='logits_cnn_cpncpnfdf')\n",
    "    else: \n",
    "        print('error not a valid model type')\n",
    "\n",
    "    return logits\n",
    "\n",
    "def compute_cross_entropy(logits, y):\n",
    "    # Compute the average cross-entropy across all the examples.\n",
    "    numerical_instability_example = 0\n",
    "    if numerical_instability_example:\n",
    "        y_pred = tf.nn.softmax(logits, name='y_pred') # the predicted probability for each example.\n",
    "        cross_ent = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_pred), reduction_indices=[1]))\n",
    "    else:\n",
    "        sm_ce = tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=logits, name='cross_ent_terms')\n",
    "        cross_ent = tf.reduce_mean(sm_ce, name='cross_ent')\n",
    "    return cross_ent\n",
    "\n",
    "def compute_accuracy(logits, y):\n",
    "    prediction = tf.argmax(logits, 1, name='pred_class')\n",
    "    true_label = tf.argmax(y, 1, name='true_class')\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_label), tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choose case to run \n",
    "opt_method = 'sgd'\n",
    "model_type = 'lr' \n",
    "dir_name = 'logs/scratch04x/{}_{}_ms{}'.format(model_type, opt_method, mean_subtract)\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   0: training accuracy 0.0580\n",
      "    sample pred: [0 8 3 3 0 8 8 8 8 3 8 0 3 0 8 8 3 0 8 3]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [15  0  2 15  0  0  0  0 25  1]\n",
      "Step   0: val accuracy 0.0758\n",
      "Step 100: training accuracy 0.2090\n",
      "    sample pred: [1 1 1 1 1 3 1 3 1 1 1 1 1 1 1 1 4 1 1 4]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  6 126  30  12  13   9   6   3   4   0]\n",
      "Step 100: val accuracy 0.2170\n",
      "Step 200: training accuracy 0.2450\n",
      "    sample pred: [2 1 3 2 1 3 1 8 3 2 2 1 3 1 1 1 4 1 1 4]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 16 118  31  23  34  13   3   0   7   0]\n",
      "Step 200: val accuracy 0.2476\n",
      "Step 300: training accuracy 0.2470\n",
      "    sample pred: [3 9 2 2 2 3 1 3 2 3 1 2 3 1 3 1 4 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  9 106  45  29  29  17   3   1   5   3]\n",
      "Step 300: val accuracy 0.2290\n",
      "Step 400: training accuracy 0.2610\n",
      "    sample pred: [1 1 2 1 1 1 1 3 2 1 1 1 3 1 1 1 1 1 1 7]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  4 141  46  25  14  11   7   5   6   2]\n",
      "Step 400: val accuracy 0.2378\n",
      "Step 500: training accuracy 0.2520\n",
      "    sample pred: [1 1 2 2 1 1 1 3 2 1 1 1 4 1 2 1 1 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  6 118  53  25  17  17   8   4   3   1]\n",
      "Step 500: val accuracy 0.2454\n",
      "Step 600: training accuracy 0.2580\n",
      "    sample pred: [1 1 0 1 1 3 1 3 9 3 1 1 1 1 1 1 1 1 1 4]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  5 136  40  19  21  21   9   2   3   2]\n",
      "Step 600: val accuracy 0.2396\n",
      "Step 700: training accuracy 0.2620\n",
      "    sample pred: [1 1 2 1 1 1 1 3 2 1 1 1 1 1 1 1 1 2 1 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 14 137  39  29  19  12   3   2   5   2]\n",
      "Step 700: val accuracy 0.2488\n",
      "Step 800: training accuracy 0.2580\n",
      "    sample pred: [1 1 2 2 2 7 1 2 1 1 2 1 1 1 1 1 4 1 2 7]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  7 112  54  18  25  19   7   2   9   5]\n",
      "Step 800: val accuracy 0.2450\n",
      "Step 900: training accuracy 0.2650\n",
      "    sample pred: [3 1 0 1 2 1 1 3 1 1 2 1 1 4 1 4 2 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 10 115  45  28  35  15   7   2   7   1]\n",
      "Step 900: val accuracy 0.2534\n",
      "Step 1000: training accuracy 0.2590\n",
      "    sample pred: [3 1 3 1 2 3 1 2 3 2 2 1 3 1 1 1 4 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 13 111  41  30  25  18   9   6   2   4]\n",
      "Step 1000: val accuracy 0.2434\n",
      "Step 1100: training accuracy 0.2710\n",
      "    sample pred: [1 1 2 2 1 5 1 3 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  1 136  41  26  15  26  10   6   7   3]\n",
      "Step 1100: val accuracy 0.2532\n",
      "Step 1200: training accuracy 0.2780\n",
      "    sample pred: [1 1 6 1 1 8 1 3 1 1 1 1 1 1 1 1 4 1 1 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  4 127  41  31  33  22   7   3   9   1]\n",
      "Step 1200: val accuracy 0.2698\n",
      "Step 1300: training accuracy 0.2560\n",
      "    sample pred: [5 1 2 1 2 3 1 3 1 1 2 1 1 1 1 2 4 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  7 129  44  24  16  14   8   4   4   6]\n",
      "Step 1300: val accuracy 0.2380\n",
      "Step 1400: training accuracy 0.2640\n",
      "    sample pred: [3 4 3 1 1 3 1 3 2 1 1 2 3 1 3 4 4 1 3 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  2 113  52  29  31  15  10   3   6   3]\n",
      "Step 1400: val accuracy 0.2528\n",
      "Step 1500: training accuracy 0.2520\n",
      "    sample pred: [1 1 2 1 1 3 2 6 2 1 1 1 1 1 1 1 1 1 2 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  1 116  51  23  16  24   6   4   6   5]\n",
      "Step 1500: val accuracy 0.2452\n",
      "Step 1600: training accuracy 0.2860\n",
      "    sample pred: [1 1 2 2 2 3 1 3 1 1 2 4 2 1 1 4 1 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  6 112  54  30  38  17   8   6  12   3]\n",
      "Step 1600: val accuracy 0.2498\n",
      "Step 1700: training accuracy 0.2610\n",
      "    sample pred: [3 1 2 2 1 7 1 3 2 3 1 2 2 4 2 1 4 1 2 3]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  6 126  48  22  24  16   8   2   8   1]\n",
      "Step 1700: val accuracy 0.2496\n",
      "Step 1800: training accuracy 0.2830\n",
      "    sample pred: [1 1 2 1 1 5 1 3 1 1 1 1 2 1 2 1 4 2 2 7]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 14 120  57  28  21  26   6   4   5   2]\n",
      "Step 1800: val accuracy 0.2688\n",
      "Step 1900: training accuracy 0.2820\n",
      "    sample pred: [1 1 2 2 1 8 1 3 1 1 1 1 1 1 1 1 4 1 1 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 18 148  43  15  20  19   8   6   3   2]\n",
      "Step 1900: val accuracy 0.2578\n",
      "Step 2000: training accuracy 0.2850\n",
      "    sample pred: [3 1 2 2 1 5 1 1 2 1 2 2 2 1 2 4 4 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 12 121  49  24  39  19  12   3   4   2]\n",
      "Step 2000: val accuracy 0.2692\n",
      "Step 2100: training accuracy 0.2570\n",
      "    sample pred: [3 1 1 2 1 3 1 3 1 2 2 1 1 1 1 1 4 1 2 6]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  3 121  47  25  24  17   8   4   5   3]\n",
      "Step 2100: val accuracy 0.2384\n",
      "Step 2200: training accuracy 0.2740\n",
      "    sample pred: [3 1 1 2 1 5 1 3 1 1 1 1 1 1 1 1 1 1 2 7]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 14 116  47  17  28  27   8   5  10   2]\n",
      "Step 2200: val accuracy 0.2592\n",
      "Step 2300: training accuracy 0.2690\n",
      "    sample pred: [3 1 3 2 1 1 1 1 1 1 2 4 3 1 1 4 4 1 3 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  5 116  36  25  50  16   9   6   4   2]\n",
      "Step 2300: val accuracy 0.2556\n",
      "Step 2400: training accuracy 0.2680\n",
      "    sample pred: [3 1 2 2 1 7 1 3 3 7 1 3 3 1 2 4 4 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  2 109  53  23  26  27   8   8   8   4]\n",
      "Step 2400: val accuracy 0.2456\n",
      "Step 2500: training accuracy 0.2660\n",
      "    sample pred: [3 1 1 2 1 8 1 3 1 1 1 1 1 1 1 1 4 1 1 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  7 124  43  25  23  15   5   6  10   8]\n",
      "Step 2500: val accuracy 0.2458\n",
      "Step 2600: training accuracy 0.2710\n",
      "    sample pred: [3 1 2 2 3 5 1 6 2 9 1 4 2 1 1 4 4 2 2 6]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  5 103  52  24  37  21  10   9   4   6]\n",
      "Step 2600: val accuracy 0.2640\n",
      "Step 2700: training accuracy 0.2460\n",
      "    sample pred: [3 1 3 2 2 8 1 3 1 3 2 1 3 1 2 4 4 1 2 6]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  3 118  47  21  20  12  11   5   4   5]\n",
      "Step 2700: val accuracy 0.2498\n",
      "Step 2800: training accuracy 0.2740\n",
      "    sample pred: [3 1 3 1 1 8 1 3 1 3 2 1 1 1 1 4 4 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  9 119  45  29  33  20   7   5   7   0]\n",
      "Step 2800: val accuracy 0.2536\n",
      "Step 2900: training accuracy 0.2670\n",
      "    sample pred: [3 1 2 1 2 8 1 3 1 1 2 1 2 1 1 1 2 1 2 7]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 13 117  57  24  10  22  12   2   7   3]\n",
      "Step 2900: val accuracy 0.2498\n",
      "Step 3000: training accuracy 0.2550\n",
      "    sample pred: [3 3 2 2 1 5 1 1 1 1 1 1 3 1 1 1 1 2 1 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 13 120  43  21  15  22   5   7   7   2]\n",
      "Step 3000: val accuracy 0.2472\n",
      "Step 3100: training accuracy 0.2730\n",
      "    sample pred: [3 1 2 2 1 5 1 9 3 7 4 8 2 1 3 4 4 2 2 6]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 4 99 56 30 38 27  5 11  3  0]\n",
      "Step 3100: val accuracy 0.2584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3200: training accuracy 0.2860\n",
      "    sample pred: [3 1 2 2 1 5 1 3 2 1 2 4 2 1 2 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  6 110  50  33  39  27   5   7   6   3]\n",
      "Step 3200: val accuracy 0.2648\n",
      "Step 3300: training accuracy 0.2670\n",
      "    sample pred: [5 1 2 1 1 3 1 3 2 1 1 4 2 1 3 1 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  3 116  56  35  26  10   6   9   5   1]\n",
      "Step 3300: val accuracy 0.2556\n",
      "Step 3400: training accuracy 0.2620\n",
      "    sample pred: [3 1 2 1 2 8 1 2 1 1 2 1 1 1 3 4 4 1 2 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  6 115  55  21  29  16   9   5   3   3]\n",
      "Step 3400: val accuracy 0.2512\n",
      "Step 3500: training accuracy 0.2510\n",
      "    sample pred: [3 1 2 1 1 3 1 3 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  5 111  47  23  26  20   8   6   3   2]\n",
      "Step 3500: val accuracy 0.2508\n",
      "Step 3600: training accuracy 0.2740\n",
      "    sample pred: [3 1 2 1 1 8 1 2 1 1 2 1 3 1 1 1 1 1 3 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  8 119  40  25  33  21   9  10   8   1]\n",
      "Step 3600: val accuracy 0.2550\n",
      "Step 3700: training accuracy 0.2820\n",
      "    sample pred: [3 1 1 5 2 7 1 1 1 1 1 2 1 1 1 4 4 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  5 127  52  21  32  17  14   7   4   3]\n",
      "Step 3700: val accuracy 0.2462\n",
      "Step 3800: training accuracy 0.2810\n",
      "    sample pred: [3 4 3 2 1 7 1 1 8 1 7 4 7 1 1 4 1 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  4 125  46  21  34  25  11   7   5   3]\n",
      "Step 3800: val accuracy 0.2654\n",
      "Step 3900: training accuracy 0.2720\n",
      "    sample pred: [5 4 8 9 2 8 1 1 1 1 1 1 3 1 1 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 11 114  50  25  37  15   8   5   3   4]\n",
      "Step 3900: val accuracy 0.2648\n",
      "Step 4000: training accuracy 0.2710\n",
      "    sample pred: [3 4 8 2 1 7 1 1 3 1 1 2 2 1 1 1 1 2 3 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 14 122  54  26  18  12   6   6  10   3]\n",
      "Step 4000: val accuracy 0.2540\n",
      "Step 4100: training accuracy 0.2730\n",
      "    sample pred: [3 1 2 9 2 3 1 3 1 1 2 2 1 4 1 4 4 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 11 115  49  22  37  16   9   9   5   0]\n",
      "Step 4100: val accuracy 0.2480\n",
      "Step 4200: training accuracy 0.2790\n",
      "    sample pred: [3 1 2 2 1 1 1 9 1 1 1 1 1 1 1 1 1 3 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 15 133  48  21  20  22   5   6   5   4]\n",
      "Step 4200: val accuracy 0.2566\n",
      "Step 4300: training accuracy 0.2840\n",
      "    sample pred: [2 1 2 2 2 5 1 3 1 1 1 1 2 1 1 4 4 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  7 120  56  24  21  32   3   8   6   7]\n",
      "Step 4300: val accuracy 0.2626\n",
      "Step 4400: training accuracy 0.2750\n",
      "    sample pred: [3 1 2 2 1 6 1 1 2 1 1 1 1 1 1 1 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  9 135  46  17  25  16   8   6   7   6]\n",
      "Step 4400: val accuracy 0.2500\n",
      "Step 4500: training accuracy 0.2890\n",
      "    sample pred: [3 1 2 2 2 5 1 3 2 1 2 2 2 1 2 4 4 2 2 5]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 10 121  51  33  27  29   8   5   3   2]\n",
      "Step 4500: val accuracy 0.2526\n",
      "Step 4600: training accuracy 0.2730\n",
      "    sample pred: [3 1 2 1 1 5 1 8 1 1 1 1 7 1 1 1 1 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  6 123  51  14  26  26   7  11   7   2]\n",
      "Step 4600: val accuracy 0.2582\n",
      "Step 4700: training accuracy 0.2620\n",
      "    sample pred: [3 1 2 2 1 3 1 9 2 1 2 2 3 1 1 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  4 120  45  18  30  23   4   5   7   6]\n",
      "Step 4700: val accuracy 0.2550\n",
      "Step 4800: training accuracy 0.2790\n",
      "    sample pred: [3 1 2 2 1 3 1 3 2 3 2 2 3 1 1 2 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 12 132  50  29  15  16  11   5   7   2]\n",
      "Step 4800: val accuracy 0.2574\n",
      "Step 4900: training accuracy 0.2950\n",
      "    sample pred: [3 1 2 2 1 8 1 3 1 1 1 1 2 1 1 4 4 1 1 8]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  9 131  48  20  31  24   8   6  13   5]\n",
      "Step 4900: val accuracy 0.2672\n",
      "Step 5000: training accuracy 0.2820\n",
      "    sample pred: [3 1 2 2 1 5 1 8 1 2 2 1 2 1 1 4 4 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 12 129  52  13  26  26   7   5   5   7]\n",
      "Step 5000: val accuracy 0.2580\n",
      "Step 5100: training accuracy 0.2470\n",
      "    sample pred: [3 1 3 2 2 8 1 3 1 2 2 2 3 1 3 2 4 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  5 101  53  29  23  15   6   4   7   4]\n",
      "Step 5100: val accuracy 0.2354\n",
      "Step 5200: training accuracy 0.2930\n",
      "    sample pred: [3 1 2 2 1 8 1 1 1 1 7 4 2 4 1 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 14 112  49  24  45  16   5   8  13   7]\n",
      "Step 5200: val accuracy 0.2758\n",
      "Step 5300: training accuracy 0.2690\n",
      "    sample pred: [3 1 8 2 1 8 1 2 8 2 1 1 8 1 1 1 4 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 15 113  45  25  31   8  10  11   5   6]\n",
      "Step 5300: val accuracy 0.2612\n",
      "Step 5400: training accuracy 0.2490\n",
      "    sample pred: [3 1 2 2 2 8 1 8 3 1 2 2 3 1 3 4 4 3 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  7 115  45  24  26  13   3   4   8   4]\n",
      "Step 5400: val accuracy 0.2456\n",
      "Step 5500: training accuracy 0.2770\n",
      "    sample pred: [3 1 2 2 2 7 1 3 2 2 2 2 2 1 1 4 4 1 2 8]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 16 116  49  20  31  14   8   8  10   5]\n",
      "Step 5500: val accuracy 0.2512\n",
      "Step 5600: training accuracy 0.2810\n",
      "    sample pred: [3 1 2 2 1 8 1 1 2 1 1 2 3 1 1 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  9 136  50  21  24  15  11   7   7   1]\n",
      "Step 5600: val accuracy 0.2532\n",
      "Step 5700: training accuracy 0.3030\n",
      "    sample pred: [3 4 2 2 1 8 1 3 2 1 1 1 2 1 1 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 10 134  58  31  30  18  10   3   6   3]\n",
      "Step 5700: val accuracy 0.2608\n",
      "Step 5800: training accuracy 0.2710\n",
      "    sample pred: [3 1 2 2 1 8 1 3 2 1 2 2 2 1 1 1 4 2 2 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 11 120  49  25  23  23   6   6   5   3]\n",
      "Step 5800: val accuracy 0.2574\n",
      "Step 5900: training accuracy 0.2860\n",
      "    sample pred: [3 4 2 2 1 8 1 3 2 1 2 1 2 1 2 4 4 3 2 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 10 111  51  28  33  26   7   9   6   5]\n",
      "Step 5900: val accuracy 0.2474\n",
      "Step 6000: training accuracy 0.2890\n",
      "    sample pred: [3 1 2 2 3 5 1 5 3 2 2 2 6 1 1 4 4 1 2 7]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 13 116  43  30  32  30  16   2   5   2]\n",
      "Step 6000: val accuracy 0.2528\n",
      "Step 6100: training accuracy 0.2840\n",
      "    sample pred: [3 4 2 1 2 3 1 1 2 1 1 1 3 4 3 4 1 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  6 112  56  26  44  20   7   5   6   2]\n",
      "Step 6100: val accuracy 0.2596\n",
      "Step 6200: training accuracy 0.2730\n",
      "    sample pred: [3 4 3 2 1 8 1 2 8 2 2 2 8 1 1 4 4 1 2 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  8 113  52  23  36  19   6   7   8   1]\n",
      "Step 6200: val accuracy 0.2582\n",
      "Step 6300: training accuracy 0.2730\n",
      "    sample pred: [3 1 3 2 1 1 1 1 2 1 1 4 3 1 1 4 4 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  8 134  45  21  33  15   3   4   7   3]\n",
      "Step 6300: val accuracy 0.2686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6400: training accuracy 0.2760\n",
      "    sample pred: [3 4 2 2 1 8 1 3 1 1 1 1 1 1 1 1 1 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  9 125  50  31  22  19   6   6   8   0]\n",
      "Step 6400: val accuracy 0.2492\n",
      "Step 6500: training accuracy 0.2980\n",
      "    sample pred: [3 1 2 1 1 7 1 1 2 1 2 2 2 1 1 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 12 122  66  20  31  27   8   7   3   2]\n",
      "Step 6500: val accuracy 0.2648\n",
      "Step 6600: training accuracy 0.2700\n",
      "    sample pred: [3 1 2 2 1 3 1 3 1 1 1 1 1 1 1 4 6 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  9 129  44  22  15  14  14   7   9   7]\n",
      "Step 6600: val accuracy 0.2536\n",
      "Step 6700: training accuracy 0.2720\n",
      "    sample pred: [3 1 2 1 1 3 1 6 1 1 1 1 3 1 1 4 6 1 1 6]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 10 118  44  21  31  21  13   9   3   2]\n",
      "Step 6700: val accuracy 0.2626\n",
      "Step 6800: training accuracy 0.2650\n",
      "    sample pred: [3 4 3 1 1 1 1 3 1 1 1 1 3 1 1 1 1 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  6 127  34  34  25  21   8   3   2   5]\n",
      "Step 6800: val accuracy 0.2580\n",
      "Step 6900: training accuracy 0.2770\n",
      "    sample pred: [3 1 3 2 1 7 1 1 3 1 1 1 3 1 1 4 1 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 14 122  40  22  25  27   6   8   8   5]\n",
      "Step 6900: val accuracy 0.2580\n",
      "Step 7000: training accuracy 0.2940\n",
      "    sample pred: [5 4 2 2 2 5 1 2 8 2 2 1 8 1 1 4 4 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 13 121  51  24  31  27   6   7  10   4]\n",
      "Step 7000: val accuracy 0.2526\n",
      "Step 7100: training accuracy 0.2790\n",
      "    sample pred: [3 4 2 1 1 5 1 5 3 5 1 1 3 1 3 4 1 1 2 5]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 15 106  48  30  28  20  10  15   6   1]\n",
      "Step 7100: val accuracy 0.2684\n",
      "Step 7200: training accuracy 0.2720\n",
      "    sample pred: [3 1 2 1 1 3 1 8 1 1 1 1 2 1 1 2 1 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  9 124  50  19  26  26   7   5   5   1]\n",
      "Step 7200: val accuracy 0.2602\n",
      "Step 7300: training accuracy 0.2780\n",
      "    sample pred: [3 1 2 2 1 3 1 5 2 1 1 1 2 1 3 1 4 3 2 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  5 117  48  40  31  19   6   4   4   4]\n",
      "Step 7300: val accuracy 0.2600\n",
      "Step 7400: training accuracy 0.2660\n",
      "    sample pred: [3 1 2 2 3 5 1 2 1 2 2 1 1 1 1 1 4 1 2 3]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 11 104  49  29  23  21  11  10   6   2]\n",
      "Step 7400: val accuracy 0.2360\n",
      "Step 7500: training accuracy 0.2870\n",
      "    sample pred: [3 1 2 5 2 7 1 3 2 3 1 1 2 1 1 4 4 1 3 3]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 20 128  38  32  29  12   6   6  10   6]\n",
      "Step 7500: val accuracy 0.2588\n",
      "Step 7600: training accuracy 0.2870\n",
      "    sample pred: [3 1 2 2 1 5 1 3 9 1 1 1 8 1 1 1 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  8 117  54  23  28  26   6  11  10   4]\n",
      "Step 7600: val accuracy 0.2700\n",
      "Step 7700: training accuracy 0.2720\n",
      "    sample pred: [3 4 2 1 2 5 1 8 3 1 2 1 2 1 1 2 4 1 2 5]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  7 116  45  32  24  20   7  12   8   1]\n",
      "Step 7700: val accuracy 0.2536\n",
      "Step 7800: training accuracy 0.2870\n",
      "    sample pred: [3 1 8 9 1 5 1 3 1 1 1 1 3 1 1 1 1 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 13 135  45  31  22  17   5   4  10   5]\n",
      "Step 7800: val accuracy 0.2582\n",
      "Step 7900: training accuracy 0.2800\n",
      "    sample pred: [3 4 2 2 1 1 1 1 2 1 1 1 2 1 1 1 1 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 10 133  49  20  26  18   8   8   6   2]\n",
      "Step 7900: val accuracy 0.2654\n",
      "Step 8000: training accuracy 0.2680\n",
      "    sample pred: [3 1 2 1 1 3 1 3 1 1 1 1 1 1 1 1 1 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  4 128  45  31  19  20   6   6   6   3]\n",
      "Step 8000: val accuracy 0.2482\n",
      "Step 8100: training accuracy 0.2950\n",
      "    sample pred: [3 1 6 2 1 7 1 3 1 1 1 4 2 1 1 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  9 125  44  30  34  18   9   6  11   9]\n",
      "Step 8100: val accuracy 0.2686\n",
      "Step 8200: training accuracy 0.2790\n",
      "    sample pred: [3 4 2 2 2 3 1 3 1 2 2 2 1 1 1 2 4 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [12 93 59 28 36 25 10  5  6  5]\n",
      "Step 8200: val accuracy 0.2544\n",
      "Step 8300: training accuracy 0.2880\n",
      "    sample pred: [5 1 2 9 2 5 1 3 1 1 1 1 2 1 1 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 10 124  49  27  28  25   5   9   9   2]\n",
      "Step 8300: val accuracy 0.2674\n",
      "Step 8400: training accuracy 0.2740\n",
      "    sample pred: [3 1 3 9 1 8 1 3 3 9 1 1 6 1 1 4 4 1 2 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  8 134  37  21  23  18  11  14   5   3]\n",
      "Step 8400: val accuracy 0.2602\n",
      "Step 8500: training accuracy 0.2600\n",
      "    sample pred: [1 1 2 9 2 3 1 3 1 1 2 1 1 1 1 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  7 114  48  25  30  11   9   6   8   2]\n",
      "Step 8500: val accuracy 0.2528\n",
      "Step 8600: training accuracy 0.2800\n",
      "    sample pred: [3 1 6 2 2 3 1 2 3 1 4 1 3 1 1 4 4 1 2 6]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [12 99 49 30 44 18  8  7  9  4]\n",
      "Step 8600: val accuracy 0.2608\n",
      "Step 8700: training accuracy 0.2670\n",
      "    sample pred: [3 4 1 2 2 7 1 1 2 1 2 2 1 1 1 1 1 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  9 119  50  23  20  21   8   6   8   3]\n",
      "Step 8700: val accuracy 0.2554\n",
      "Step 8800: training accuracy 0.2740\n",
      "    sample pred: [3 1 2 2 1 1 1 3 1 1 1 1 2 1 1 1 1 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 10 124  53  26  25  13   9   5   6   3]\n",
      "Step 8800: val accuracy 0.2522\n",
      "Step 8900: training accuracy 0.2950\n",
      "    sample pred: [3 1 2 1 2 8 1 8 2 1 2 1 2 1 2 4 1 1 2 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 14 119  51  27  24  32   9   7  10   2]\n",
      "Step 8900: val accuracy 0.2564\n",
      "Step 9000: training accuracy 0.2760\n",
      "    sample pred: [1 1 3 2 1 1 1 3 1 1 1 1 1 1 1 1 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  8 135  38  27  23  20   8   5   8   4]\n",
      "Step 9000: val accuracy 0.2448\n",
      "Step 9100: training accuracy 0.2790\n",
      "    sample pred: [3 1 2 2 1 5 1 3 1 1 1 1 2 1 2 1 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  9 120  49  21  25  28   7   9   9   2]\n",
      "Step 9100: val accuracy 0.2702\n",
      "Step 9200: training accuracy 0.2960\n",
      "    sample pred: [3 1 1 2 1 8 1 3 1 1 1 1 1 1 1 4 6 1 2 3]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  6 125  43  26  38  28   7   5  10   8]\n",
      "Step 9200: val accuracy 0.2690\n",
      "Step 9300: training accuracy 0.2820\n",
      "    sample pred: [3 4 2 2 1 7 1 1 2 1 1 2 2 4 1 4 1 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  9 112  60  18  36  19   8   9   8   3]\n",
      "Step 9300: val accuracy 0.2546\n",
      "Step 9400: training accuracy 0.3010\n",
      "    sample pred: [3 1 2 2 1 8 1 3 1 1 1 1 1 1 1 4 4 1 2 8]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  8 121  48  27  29  28   9   9  14   8]\n",
      "Step 9400: val accuracy 0.2624\n",
      "Step 9500: training accuracy 0.2750\n",
      "    sample pred: [3 1 2 2 1 8 1 8 2 1 1 1 3 1 3 1 4 5 2 8]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 13 105  54  24  28  17   8   8  14   4]\n",
      "Step 9500: val accuracy 0.2620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9600: training accuracy 0.3020\n",
      "    sample pred: [3 1 6 2 2 5 1 3 2 1 2 2 2 1 2 4 4 2 2 8]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 10 115  53  35  34  29   9   4   9   4]\n",
      "Step 9600: val accuracy 0.2604\n",
      "Step 9700: training accuracy 0.2740\n",
      "    sample pred: [3 4 2 2 1 3 1 5 2 1 1 2 3 1 1 4 6 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 11 100  44  32  30  17  10  11  11   8]\n",
      "Step 9700: val accuracy 0.2584\n",
      "Step 9800: training accuracy 0.3020\n",
      "    sample pred: [3 1 2 2 1 5 1 5 2 1 1 2 2 1 1 4 4 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  9 130  49  23  37  20  12   9   7   6]\n",
      "Step 9800: val accuracy 0.2654\n",
      "Step 9900: training accuracy 0.3010\n",
      "    sample pred: [3 1 2 2 1 5 1 3 1 1 1 1 3 5 1 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 14 119  52  18  42  27   9   8   9   3]\n",
      "Step 9900: val accuracy 0.2732\n",
      "Step 10000: training accuracy 0.2890\n",
      "    sample pred: [4 1 0 2 2 5 1 3 1 1 2 1 2 1 1 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 13 114  55  22  35  22   5   7   7   9]\n",
      "Step 10000: val accuracy 0.2700\n",
      "Step 10100: training accuracy 0.2710\n",
      "    sample pred: [3 4 2 2 1 3 1 1 2 1 1 4 3 1 3 4 4 5 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 15 104  40  31  38  20   6   6   8   3]\n",
      "Step 10100: val accuracy 0.2580\n",
      "Step 10200: training accuracy 0.2910\n",
      "    sample pred: [3 1 2 2 1 3 1 1 2 2 1 1 1 1 3 4 4 1 1 4]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 13 124  54  26  33  17   7  10   5   2]\n",
      "Step 10200: val accuracy 0.2614\n",
      "Step 10300: training accuracy 0.2800\n",
      "    sample pred: [3 1 1 2 1 8 1 8 1 1 1 1 2 1 1 4 4 1 1 8]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 13 121  41  27  32  14   9   7  14   2]\n",
      "Step 10300: val accuracy 0.2558\n",
      "Step 10400: training accuracy 0.2670\n",
      "    sample pred: [3 1 1 1 1 3 1 8 1 3 1 1 1 1 1 1 4 1 2 6]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  7 117  46  26  19  20  11   9  10   2]\n",
      "Step 10400: val accuracy 0.2488\n",
      "Step 10500: training accuracy 0.2880\n",
      "    sample pred: [4 1 6 1 1 8 1 1 1 1 1 1 2 1 1 1 4 5 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 14 121  45  27  29  23  11   4  11   3]\n",
      "Step 10500: val accuracy 0.2618\n",
      "Step 10600: training accuracy 0.2900\n",
      "    sample pred: [3 1 1 1 2 3 1 8 1 1 2 1 1 1 1 4 4 1 2 8]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  9 121  57  19  37  16   6   6  12   7]\n",
      "Step 10600: val accuracy 0.2642\n",
      "Step 10700: training accuracy 0.2880\n",
      "    sample pred: [3 4 2 2 1 3 1 3 2 2 2 2 1 1 1 4 4 1 2 6]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  9 113  51  26  38  18  10   9  10   4]\n",
      "Step 10700: val accuracy 0.2582\n",
      "Step 10800: training accuracy 0.2770\n",
      "    sample pred: [3 1 8 2 2 3 1 3 2 1 2 4 3 1 1 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 12 118  50  30  31   9   8  10   7   2]\n",
      "Step 10800: val accuracy 0.2644\n",
      "Step 10900: training accuracy 0.2990\n",
      "    sample pred: [3 1 1 1 2 5 1 3 1 1 2 1 3 1 1 4 4 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  9 119  55  27  42  24   6  10   6   1]\n",
      "Step 10900: val accuracy 0.2642\n",
      "Step 11000: training accuracy 0.2940\n",
      "    sample pred: [3 1 1 2 1 3 1 8 1 1 1 1 3 1 1 1 4 2 2 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 15 122  47  22  30  28   8  10   9   3]\n",
      "Step 11000: val accuracy 0.2658\n",
      "Step 11100: training accuracy 0.2890\n",
      "    sample pred: [3 1 2 2 1 5 1 3 1 9 7 1 1 1 1 1 1 1 2 6]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  9 119  47  31  27  28  10  11   6   1]\n",
      "Step 11100: val accuracy 0.2474\n",
      "Step 11200: training accuracy 0.2960\n",
      "    sample pred: [3 1 2 2 2 5 1 8 2 2 1 1 1 1 1 1 4 1 2 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  9 114  43  28  38  33   6   3  13   9]\n",
      "Step 11200: val accuracy 0.2646\n",
      "Step 11300: training accuracy 0.2950\n",
      "    sample pred: [3 1 8 2 1 5 1 9 1 1 1 1 1 1 1 4 4 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 13 143  34  29  17  26   8   4  13   8]\n",
      "Step 11300: val accuracy 0.2574\n",
      "Step 11400: training accuracy 0.2670\n",
      "    sample pred: [3 1 2 2 1 8 1 3 1 9 2 1 2 1 1 1 4 1 2 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  8 106  45  29  31  14  13   7   8   6]\n",
      "Step 11400: val accuracy 0.2598\n",
      "Step 11500: training accuracy 0.2620\n",
      "    sample pred: [3 4 2 2 1 8 1 3 1 1 1 1 2 3 1 1 1 3 1 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 10 117  35  31  16  31   6   6   7   3]\n",
      "Step 11500: val accuracy 0.2560\n",
      "Step 11600: training accuracy 0.2770\n",
      "    sample pred: [3 1 8 2 1 5 1 3 1 1 1 1 1 1 1 4 6 1 2 6]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 14 134  40  31  21  14  10   6   6   1]\n",
      "Step 11600: val accuracy 0.2562\n",
      "Step 11700: training accuracy 0.2640\n",
      "    sample pred: [3 4 2 2 1 8 1 3 2 1 1 1 6 1 1 1 1 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 12 125  50  18  20  14   3   8  10   4]\n",
      "Step 11700: val accuracy 0.2634\n",
      "Step 11800: training accuracy 0.2930\n",
      "    sample pred: [3 1 2 9 1 3 1 3 1 7 3 4 2 1 1 4 4 1 2 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 10 116  53  36  42  10   9   9   5   3]\n",
      "Step 11800: val accuracy 0.2628\n",
      "Step 11900: training accuracy 0.2820\n",
      "    sample pred: [3 1 3 2 2 3 1 3 1 1 1 1 2 1 1 1 1 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  8 111  61  25  24  18  15   6   8   6]\n",
      "Step 11900: val accuracy 0.2578\n",
      "Step 12000: training accuracy 0.2870\n",
      "    sample pred: [3 1 2 2 1 3 1 1 2 1 1 4 3 1 1 4 4 3 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 12 117  49  26  29  24  14   5   5   6]\n",
      "Step 12000: val accuracy 0.2720\n",
      "Step 12100: training accuracy 0.2840\n",
      "    sample pred: [3 1 2 2 1 3 1 3 2 1 2 2 2 1 1 4 4 1 2 3]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 11 116  51  29  34  20  10   4   6   3]\n",
      "Step 12100: val accuracy 0.2550\n",
      "Step 12200: training accuracy 0.2900\n",
      "    sample pred: [3 1 2 2 2 8 1 1 2 1 1 1 2 1 2 1 4 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  8 119  58  28  30  15  13  11   6   2]\n",
      "Step 12200: val accuracy 0.2508\n",
      "Step 12300: training accuracy 0.2900\n",
      "    sample pred: [3 4 2 1 2 3 1 5 1 1 1 1 1 1 1 4 1 1 2 6]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 11 125  44  30  35  20   5   7   5   8]\n",
      "Step 12300: val accuracy 0.2656\n",
      "Step 12400: training accuracy 0.2920\n",
      "    sample pred: [3 4 2 2 1 3 1 3 2 1 1 2 2 1 2 1 1 2 2 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 12 103  52  40  27  27  10  11   6   4]\n",
      "Step 12400: val accuracy 0.2528\n",
      "Step 12500: training accuracy 0.2670\n",
      "    sample pred: [3 4 2 2 1 5 1 1 2 1 1 4 2 4 1 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  6 114  49  21  29  28   8   7   4   1]\n",
      "Step 12500: val accuracy 0.2454\n",
      "Step 12600: training accuracy 0.2810\n",
      "    sample pred: [3 1 2 2 1 3 1 1 3 3 1 1 3 1 1 1 4 1 1 3]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 11 132  42  24  22  13  12  12  11   2]\n",
      "Step 12600: val accuracy 0.2604\n",
      "Step 12700: training accuracy 0.2780\n",
      "    sample pred: [3 1 2 2 2 8 1 3 1 1 1 1 3 1 1 1 1 1 2 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  9 130  46  25  21  17  10  10   8   2]\n",
      "Step 12700: val accuracy 0.2486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12800: training accuracy 0.2870\n",
      "    sample pred: [3 1 2 1 2 3 1 3 2 1 2 2 1 1 1 4 6 1 2 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 12 112  48  30  37  23  13   5   5   2]\n",
      "Step 12800: val accuracy 0.2516\n",
      "Step 12900: training accuracy 0.2870\n",
      "    sample pred: [3 1 1 1 2 8 1 3 1 1 2 1 3 1 1 1 4 1 2 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  8 131  48  22  32  16   9   6  11   4]\n",
      "Step 12900: val accuracy 0.2736\n",
      "Step 13000: training accuracy 0.2840\n",
      "    sample pred: [3 1 2 9 2 3 1 3 2 1 2 1 2 1 3 4 4 1 2 8]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  8 135  50  18  34  13   8   8   9   1]\n",
      "Step 13000: val accuracy 0.2690\n",
      "Step 13100: training accuracy 0.2780\n",
      "    sample pred: [3 4 2 2 2 3 1 8 1 1 2 2 2 1 1 1 1 1 1 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 11 117  53  24  24  21   7   9  10   2]\n",
      "Step 13100: val accuracy 0.2538\n",
      "Step 13200: training accuracy 0.2990\n",
      "    sample pred: [3 1 6 1 1 3 1 3 1 1 1 1 1 1 1 1 1 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 11 132  41  28  31  19  10   5  13   9]\n",
      "Step 13200: val accuracy 0.2596\n",
      "Step 13300: training accuracy 0.2660\n",
      "    sample pred: [3 4 2 2 1 8 1 3 2 1 1 1 2 1 1 1 4 2 2 8]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [12 99 50 28 29 15  7  5 13  8]\n",
      "Step 13300: val accuracy 0.2540\n",
      "Step 13400: training accuracy 0.3030\n",
      "    sample pred: [3 1 2 2 1 7 1 3 1 7 1 1 2 1 1 1 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 17 141  45  27  23  24   6   7   8   5]\n",
      "Step 13400: val accuracy 0.2640\n",
      "Step 13500: training accuracy 0.2730\n",
      "    sample pred: [3 1 2 2 1 8 1 3 2 9 2 1 2 1 2 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 12 106  55  27  32  16   9   6   7   3]\n",
      "Step 13500: val accuracy 0.2598\n",
      "Step 13600: training accuracy 0.2870\n",
      "    sample pred: [3 1 1 2 2 7 1 3 1 1 2 1 1 1 1 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 14 112  48  31  38  19   3   9   8   5]\n",
      "Step 13600: val accuracy 0.2606\n",
      "Step 13700: training accuracy 0.2850\n",
      "    sample pred: [3 1 2 2 2 8 1 3 2 9 2 1 2 1 1 4 4 2 2 5]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 10 116  54  31  26  16  10  10   9   3]\n",
      "Step 13700: val accuracy 0.2538\n",
      "Step 13800: training accuracy 0.2600\n",
      "    sample pred: [3 1 3 1 2 1 1 3 1 1 1 1 1 1 1 1 1 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  7 116  48  20  21  19   9   8  10   2]\n",
      "Step 13800: val accuracy 0.2474\n",
      "Step 13900: training accuracy 0.2770\n",
      "    sample pred: [3 1 3 2 1 1 1 8 2 1 1 1 2 1 1 1 1 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 11 117  48  24  25  23  11   6   8   4]\n",
      "Step 13900: val accuracy 0.2544\n",
      "Step 14000: training accuracy 0.2790\n",
      "    sample pred: [3 1 6 2 2 1 1 3 2 1 1 1 4 1 1 4 4 2 2 6]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  7 109  39  30  34  32   9   5   8   6]\n",
      "Step 14000: val accuracy 0.2600\n",
      "Step 14100: training accuracy 0.3010\n",
      "    sample pred: [3 1 2 2 1 5 1 1 2 1 1 2 2 1 1 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 11 113  55  27  38  32   9   5   9   2]\n",
      "Step 14100: val accuracy 0.2652\n",
      "Step 14200: training accuracy 0.2920\n",
      "    sample pred: [3 1 2 1 1 3 1 3 1 1 1 1 3 1 1 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  9 127  49  28  36  24   7   5   4   3]\n",
      "Step 14200: val accuracy 0.2680\n",
      "Step 14300: training accuracy 0.2770\n",
      "    sample pred: [3 1 2 2 1 5 1 1 2 1 1 1 3 1 1 1 4 3 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  5 123  46  30  26  25   9   6   5   2]\n",
      "Step 14300: val accuracy 0.2618\n",
      "Step 14400: training accuracy 0.2930\n",
      "    sample pred: [3 1 3 1 2 7 1 3 2 1 1 1 2 1 1 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 11 120  46  23  36  25  10   7   8   7]\n",
      "Step 14400: val accuracy 0.2624\n",
      "Step 14500: training accuracy 0.2890\n",
      "    sample pred: [3 4 3 2 1 7 1 2 2 2 1 1 1 5 1 4 1 3 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  8 134  51  26  23  22   5   8   8   4]\n",
      "Step 14500: val accuracy 0.2608\n",
      "Step 14600: training accuracy 0.2810\n",
      "    sample pred: [3 1 8 1 2 7 1 1 2 1 1 1 1 1 1 4 4 2 2 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  8 135  48  21  25  19   9   6   8   2]\n",
      "Step 14600: val accuracy 0.2624\n",
      "Step 14700: training accuracy 0.2860\n",
      "    sample pred: [3 1 2 1 1 1 1 1 2 1 1 2 2 1 1 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 12 108  57  24  43  22   7   4   6   3]\n",
      "Step 14700: val accuracy 0.2558\n",
      "Step 14800: training accuracy 0.2670\n",
      "    sample pred: [3 1 2 1 1 8 1 2 1 4 2 1 1 1 1 4 4 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  5 131  46  19  20  22  12   4   5   3]\n",
      "Step 14800: val accuracy 0.2466\n",
      "Step 14900: training accuracy 0.2690\n",
      "    sample pred: [3 1 2 1 3 3 1 3 2 1 1 1 8 1 1 4 1 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  7 114  51  29  22  24  11   6   2   3]\n",
      "Step 14900: val accuracy 0.2578\n",
      "Step 15000: training accuracy 0.2900\n",
      "    sample pred: [3 1 8 1 1 5 1 3 1 1 1 4 1 1 1 4 4 2 2 6]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 11 127  46  19  34  24  10   9   6   4]\n",
      "Step 15000: val accuracy 0.2714\n",
      "Step 15100: training accuracy 0.3060\n",
      "    sample pred: [3 1 2 1 2 8 1 3 1 1 2 1 1 4 1 4 4 1 2 8]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 12 130  51  31  34  24   8   3  10   3]\n",
      "Step 15100: val accuracy 0.2658\n",
      "Step 15200: training accuracy 0.2700\n",
      "    sample pred: [3 1 2 2 1 8 1 3 2 1 1 2 1 1 1 1 1 2 2 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  8 111  51  23  26  19  12   6  10   4]\n",
      "Step 15200: val accuracy 0.2550\n",
      "Step 15300: training accuracy 0.2600\n",
      "    sample pred: [3 1 3 1 2 8 1 3 1 7 2 1 1 1 1 1 2 1 2 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 14 112  52  22  21  11   6  10  10   2]\n",
      "Step 15300: val accuracy 0.2402\n",
      "Step 15400: training accuracy 0.2860\n",
      "    sample pred: [3 1 1 1 2 3 1 5 1 3 2 2 3 1 1 4 4 1 2 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 11 116  53  29  36  21   7   4   7   2]\n",
      "Step 15400: val accuracy 0.2602\n",
      "Step 15500: training accuracy 0.3110\n",
      "    sample pred: [6 1 2 2 1 8 1 3 1 1 1 1 1 1 1 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  7 119  58  21  38  29  12  13   8   6]\n",
      "Step 15500: val accuracy 0.2604\n",
      "Step 15600: training accuracy 0.2850\n",
      "    sample pred: [3 1 2 1 1 8 1 3 1 1 2 1 0 1 1 1 1 1 2 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 10 137  43  24  25  18  11   4   7   6]\n",
      "Step 15600: val accuracy 0.2610\n",
      "Step 15700: training accuracy 0.2920\n",
      "    sample pred: [6 1 2 1 2 5 1 5 2 9 2 2 1 1 1 4 4 1 2 6]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  6 111  47  22  40  35  12   9   6   4]\n",
      "Step 15700: val accuracy 0.2576\n",
      "Step 15800: training accuracy 0.2820\n",
      "    sample pred: [5 1 3 1 2 5 1 8 1 1 2 1 1 1 1 1 1 1 1 5]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 10 139  39  28  27  13   8   5   9   4]\n",
      "Step 15800: val accuracy 0.2532\n",
      "Step 15900: training accuracy 0.3120\n",
      "    sample pred: [3 1 2 2 1 7 1 3 2 1 1 2 3 1 1 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 11 126  43  28  40  25  10  14  12   3]\n",
      "Step 15900: val accuracy 0.2708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16000: training accuracy 0.2870\n",
      "    sample pred: [3 1 2 1 1 8 1 8 1 7 1 4 1 1 1 4 4 1 2 7]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  7 119  47  23  38  18  10   9  11   5]\n",
      "Step 16000: val accuracy 0.2560\n",
      "Step 16100: training accuracy 0.2680\n",
      "    sample pred: [3 1 2 1 2 5 1 9 1 1 1 1 1 1 1 1 1 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  8 117  51  27  19  17   7   9   6   7]\n",
      "Step 16100: val accuracy 0.2554\n",
      "Step 16200: training accuracy 0.2830\n",
      "    sample pred: [5 1 2 2 1 5 1 3 8 9 1 1 8 1 3 1 1 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  9 133  46  27  24  17   7   4  11   5]\n",
      "Step 16200: val accuracy 0.2496\n",
      "Step 16300: training accuracy 0.2770\n",
      "    sample pred: [3 1 2 2 1 8 1 3 1 1 1 1 3 1 1 1 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  8 119  57  27  24  17   8   7   7   3]\n",
      "Step 16300: val accuracy 0.2644\n",
      "Step 16400: training accuracy 0.2810\n",
      "    sample pred: [3 1 2 2 1 3 1 3 3 1 1 1 3 1 3 4 4 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 11 125  52  26  22  15   6   8   9   7]\n",
      "Step 16400: val accuracy 0.2732\n",
      "Step 16500: training accuracy 0.2690\n",
      "    sample pred: [3 1 2 2 1 3 1 3 1 1 1 1 1 1 1 4 4 1 2 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  7 128  42  29  22  17   6   6   9   3]\n",
      "Step 16500: val accuracy 0.2538\n",
      "Step 16600: training accuracy 0.2750\n",
      "    sample pred: [3 1 2 2 1 5 1 3 1 2 2 2 1 1 1 4 4 1 2 6]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 11 124  48  19  26  20   7   7  11   2]\n",
      "Step 16600: val accuracy 0.2424\n",
      "Step 16700: training accuracy 0.2980\n",
      "    sample pred: [3 1 2 2 1 5 1 3 2 1 1 2 2 4 1 1 4 1 1 6]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 16 122  51  21  41  23   6   7   8   3]\n",
      "Step 16700: val accuracy 0.2756\n",
      "Step 16800: training accuracy 0.2790\n",
      "    sample pred: [3 1 3 2 1 8 1 3 3 1 1 1 3 1 1 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 11 122  44  28  27  16  11   7   8   5]\n",
      "Step 16800: val accuracy 0.2594\n",
      "Step 16900: training accuracy 0.2930\n",
      "    sample pred: [3 1 2 2 2 1 1 3 2 1 1 1 1 1 1 1 1 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 15 135  50  27  26  17   4   7  10   2]\n",
      "Step 16900: val accuracy 0.2598\n",
      "Step 17000: training accuracy 0.2780\n",
      "    sample pred: [3 1 1 2 1 3 1 3 1 1 2 2 8 1 1 1 3 2 2 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 12 112  52  34  24  23   9   4   7   1]\n",
      "Step 17000: val accuracy 0.2514\n",
      "Step 17100: training accuracy 0.2730\n",
      "    sample pred: [3 4 2 2 1 8 1 3 3 1 2 2 3 1 3 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 13 111  47  36  24  15   4   9  10   4]\n",
      "Step 17100: val accuracy 0.2548\n",
      "Step 17200: training accuracy 0.3050\n",
      "    sample pred: [3 1 2 2 1 1 1 3 1 1 1 1 2 1 1 1 1 2 2 0]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 15 127  55  30  31  19   5  11   9   3]\n",
      "Step 17200: val accuracy 0.2720\n",
      "Step 17300: training accuracy 0.2880\n",
      "    sample pred: [3 1 2 2 1 7 1 3 2 1 1 2 2 1 1 1 4 2 1 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 14 122  47  30  33  11  13   7   7   4]\n",
      "Step 17300: val accuracy 0.2696\n",
      "Step 17400: training accuracy 0.3090\n",
      "    sample pred: [3 1 2 2 1 7 1 8 2 7 1 1 2 1 1 4 4 2 2 7]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  8 138  53  34  26  13   9  13  11   4]\n",
      "Step 17400: val accuracy 0.2596\n",
      "Step 17500: training accuracy 0.2890\n",
      "    sample pred: [3 1 2 2 1 7 1 5 2 1 1 1 2 1 1 1 1 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 12 126  53  29  18  22  14   2   8   5]\n",
      "Step 17500: val accuracy 0.2650\n",
      "Step 17600: training accuracy 0.2720\n",
      "    sample pred: [3 1 2 9 1 5 1 3 2 9 1 1 1 1 1 1 1 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  5 114  54  21  25  23   9   6   9   6]\n",
      "Step 17600: val accuracy 0.2608\n",
      "Step 17700: training accuracy 0.2970\n",
      "    sample pred: [3 1 3 2 1 8 1 1 3 1 1 2 8 4 1 4 1 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 15 120  49  29  26  29  10   7   6   6]\n",
      "Step 17700: val accuracy 0.2680\n",
      "Step 17800: training accuracy 0.3120\n",
      "    sample pred: [1 1 2 9 2 3 1 8 2 9 2 2 2 1 1 1 4 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 12 145  59  20  20  24  13   4  10   5]\n",
      "Step 17800: val accuracy 0.2544\n",
      "Step 17900: training accuracy 0.2890\n",
      "    sample pred: [3 1 2 2 1 1 1 1 1 1 1 1 1 1 1 4 4 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 10 136  49  20  31  19   8   4   7   5]\n",
      "Step 17900: val accuracy 0.2652\n",
      "Step 18000: training accuracy 0.2950\n",
      "    sample pred: [3 1 2 2 2 5 1 8 1 2 2 1 1 1 1 4 4 1 2 8]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 10 115  51  30  32  26   7   7  11   6]\n",
      "Step 18000: val accuracy 0.2630\n",
      "Step 18100: training accuracy 0.3020\n",
      "    sample pred: [3 1 2 2 1 5 1 2 1 1 1 1 0 1 1 4 6 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 13 116  54  34  36  23   9   8   7   2]\n",
      "Step 18100: val accuracy 0.2594\n",
      "Step 18200: training accuracy 0.3050\n",
      "    sample pred: [3 1 2 1 1 1 1 3 1 1 1 1 1 4 1 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 13 126  49  25  44  22  10   5   5   6]\n",
      "Step 18200: val accuracy 0.2672\n",
      "Step 18300: training accuracy 0.2690\n",
      "    sample pred: [3 1 1 2 1 5 1 3 1 1 2 1 1 1 1 1 1 1 2 6]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 11 118  46  26  18  24  10   7   7   2]\n",
      "Step 18300: val accuracy 0.2410\n",
      "Step 18400: training accuracy 0.2940\n",
      "    sample pred: [3 1 6 1 2 5 1 3 1 1 2 4 2 1 1 4 4 1 2 6]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 13 117  45  33  27  21  16  13   6   3]\n",
      "Step 18400: val accuracy 0.2700\n",
      "Step 18500: training accuracy 0.2800\n",
      "    sample pred: [3 1 2 2 1 8 1 9 2 2 1 1 2 1 1 4 6 2 2 6]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 14 119  47  28  31  13  15   4   5   4]\n",
      "Step 18500: val accuracy 0.2568\n",
      "Step 18600: training accuracy 0.3000\n",
      "    sample pred: [3 1 3 9 1 8 1 9 1 1 2 1 2 1 1 4 4 1 2 8]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  8 134  50  24  39  13  10   5  16   1]\n",
      "Step 18600: val accuracy 0.2694\n",
      "Step 18700: training accuracy 0.2870\n",
      "    sample pred: [3 1 8 1 2 7 1 3 2 1 1 1 2 1 1 1 1 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 17 124  50  26  23  16   9   8  11   3]\n",
      "Step 18700: val accuracy 0.2590\n",
      "Step 18800: training accuracy 0.2990\n",
      "    sample pred: [3 4 2 2 1 3 1 2 2 1 1 1 1 1 1 1 1 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 18 123  51  24  32  21  10   5   9   6]\n",
      "Step 18800: val accuracy 0.2702\n",
      "Step 18900: training accuracy 0.3200\n",
      "    sample pred: [3 1 2 9 1 5 1 1 2 1 2 9 2 1 1 4 4 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 15 130  52  31  38  28   7   8   6   5]\n",
      "Step 18900: val accuracy 0.2666\n",
      "Step 19000: training accuracy 0.3100\n",
      "    sample pred: [3 1 2 2 1 8 1 1 1 1 1 4 2 4 1 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 17 125  52  29  38  18  15   5   8   3]\n",
      "Step 19000: val accuracy 0.2680\n",
      "Step 19100: training accuracy 0.2900\n",
      "    sample pred: [3 1 1 2 1 5 1 1 1 1 2 2 1 1 1 2 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 10 122  53  25  26  26   6   6  13   3]\n",
      "Step 19100: val accuracy 0.2528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 19200: training accuracy 0.2950\n",
      "    sample pred: [3 4 3 9 1 5 1 1 9 1 2 4 3 1 1 4 4 2 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 16 129  41  32  32  16  10   8   6   5]\n",
      "Step 19200: val accuracy 0.2672\n",
      "Step 19300: training accuracy 0.2660\n",
      "    sample pred: [3 4 3 1 1 8 1 3 1 1 1 1 5 1 1 1 1 2 1 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  6 126  42  25  20  20  10   7   8   2]\n",
      "Step 19300: val accuracy 0.2550\n",
      "Step 19400: training accuracy 0.2840\n",
      "    sample pred: [3 1 1 2 1 3 1 9 2 2 1 1 3 1 3 4 4 1 1 5]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 14 114  39  28  35  17  17   8   7   5]\n",
      "Step 19400: val accuracy 0.2594\n",
      "Step 19500: training accuracy 0.2830\n",
      "    sample pred: [3 4 2 1 1 1 1 5 1 1 1 1 1 1 1 4 1 2 1 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 11 117  47  24  34  20   8  11  11   0]\n",
      "Step 19500: val accuracy 0.2622\n",
      "Step 19600: training accuracy 0.2750\n",
      "    sample pred: [3 1 3 1 1 7 1 5 2 1 2 2 3 1 1 1 4 2 2 7]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  9 129  47  26  24   9   6  11   8   6]\n",
      "Step 19600: val accuracy 0.2540\n",
      "Step 19700: training accuracy 0.2920\n",
      "    sample pred: [3 1 2 2 1 8 1 5 2 1 1 1 2 1 1 2 4 2 2 6]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  8 133  55  24  21  21   9  10   8   3]\n",
      "Step 19700: val accuracy 0.2638\n",
      "Step 19800: training accuracy 0.2730\n",
      "    sample pred: [3 1 2 1 1 8 1 3 1 1 1 1 2 1 1 1 4 1 1 6]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  6 125  47  32  23  18   8   3   8   3]\n",
      "Step 19800: val accuracy 0.2488\n",
      "Step 19900: training accuracy 0.3050\n",
      "    sample pred: [3 1 2 2 2 5 1 3 2 1 2 2 2 1 1 4 4 2 2 3]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [ 16 112  53  41  35  20  10   4   9   5]\n",
      "Step 19900: val accuracy 0.2590\n",
      "Step 20000: training accuracy 0.3040\n",
      "    sample pred: [3 1 2 2 1 5 1 1 2 7 1 1 2 1 1 1 4 1 2 1]\n",
      "    sample true: [1 9 2 3 2 5 9 3 3 1 3 3 2 8 7 4 4 1 2 8]\n",
      "    correct predictions by class: [  7 142  45  32  22  29   7  11   4   5]\n",
      "Step 20000: val accuracy 0.2672\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    # We build the model here as before\n",
    "    x = tf.placeholder(tf.float32, [None, 32*32*3], name='x')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='y')\n",
    "    pkeep = tf.placeholder(tf.float32, name='pkeep')\n",
    "    \n",
    "    with tf.name_scope('model'):\n",
    "        logits = compute_logits(x, model_type, pkeep)\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = compute_cross_entropy(logits=logits, y=y)\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = compute_accuracy(logits, y)\n",
    "    \n",
    "    with tf.name_scope('opt'):\n",
    "        if opt_method == 'sgd':\n",
    "            opt = tf.train.GradientDescentOptimizer(0.5)\n",
    "        elif opt_method == 'rms':\n",
    "            opt = tf.train.RMSPropOptimizer(.001)\n",
    "        elif opt_method == 'adam':\n",
    "            opt = tf.train.AdamOptimizer(1e-4)\n",
    "        train_step = opt.minimize(loss)\n",
    "    \n",
    "    with tf.name_scope('summaries'):\n",
    "        # create summary for loss and accuracy\n",
    "        tf.summary.scalar('loss', loss) \n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        # create summary for logits\n",
    "        tf.summary.histogram('logits', logits)\n",
    "        # create summary for input image\n",
    "        tf.summary.image('input', tf.reshape(x, [-1, 32, 32, 3]))\n",
    "    \n",
    "        summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        summary_writer = tf.summary.FileWriter(dir_name, sess.graph)\n",
    "        summary_writer_train = tf.summary.FileWriter(dir_name+'/train', sess.graph)\n",
    "        summary_writer_val = tf.summary.FileWriter(dir_name+'/val')\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        for i in range(20001):\n",
    "            batch = np.floor(np.random.rand(batch_size)*(n-n_val)).astype(int)\n",
    "            X_batch = X_train[:,:,:,batch].transpose([3,0,1,2]).reshape([batch_size,-1])\n",
    "            y_batch = y_onehot[batch]\n",
    "\n",
    "            # now run\n",
    "            _ , summary = sess.run((train_step, summary_op),\n",
    "                                      feed_dict={x: X_batch, y: y_batch, pkeep:0.85})\n",
    "            \n",
    "            # write the summary output to file\n",
    "            if i%100==0:\n",
    "                summary_writer_train.add_summary(summary, i)\n",
    "\n",
    "            # print diagnostics\n",
    "            if i%100 == 0:\n",
    "                X_batch = X_train[:,:,:,0:1000].transpose([3,0,1,2]).reshape([1000,-1])\n",
    "                y_batch = y_onehot[0:1000]\n",
    "                (train_error,train_logits) = sess.run((accuracy,logits), {x: X_batch, y: y_batch, pkeep:1.0})\n",
    "                print(\"\\rStep {0:3d}: training accuracy {1:0.4f}\".format(i, train_error), flush=True)\n",
    "                # further diagnostics\n",
    "                perf_eval(train_logits, y_batch)\n",
    "                \n",
    "            if i%100 == 0:\n",
    "                X_batch = X_val.transpose([3,0,1,2]).reshape([n_val,-1])\n",
    "                y_batch = y_onehot_val\n",
    "                (val_error, summary) = sess.run((accuracy,summary_op), {x:X_batch, y:y_batch, pkeep:1.0})\n",
    "                print(\"\\rStep {0:3d}: val accuracy {1:0.4f}\".format(i, val_error), flush=True)\n",
    "                summary_writer_val.add_summary(summary, i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide extras; no didactic purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x132e0d2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot different color channels \n",
    "showr = np.zeros_like(mat_train['X'][:,:,:,1])\n",
    "showr[:,:,0] = mat_train['X'][:,:,0,1]\n",
    "plot_save(showr, 'svhn_digit_0x', cmap=None, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
