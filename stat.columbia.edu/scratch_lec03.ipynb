{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced ML Part II // Lecture 03 Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# written by John P. Cunningham, for use in lecture\n",
    "# continues many of the conventions set out in Wenda Zhou's excellent tf tutorial\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and sample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# get mnist data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('../data/mnist', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cnn conv stuff\n",
    "def conv(x, W):\n",
    "    \"\"\"simple wrapper for tf.nn.conv2d\"\"\"\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def maxpool(x):\n",
    "    \"\"\"simple wrapper for tf.nn.max_pool with stride size 2\"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elaborate the compute_logits code to include a variety of models\n",
    "def compute_logits(x, model_type, pkeep):\n",
    "    \"\"\"Compute the logits of the model\"\"\"\n",
    "    if model_type=='lr':\n",
    "        W = tf.get_variable('W', shape=[28*28, 10])\n",
    "        b = tf.get_variable('b', shape=[10])\n",
    "        logits = tf.add(tf.matmul(x, W), b, name='logits_lr')\n",
    "    elif model_type=='cnn_cf':\n",
    "        # try a 1 layer cnn\n",
    "        n1 = 64\n",
    "        x_image = tf.reshape(x, [-1,28,28,1]) # batch, then width, height, channels\n",
    "        # cnn layer 1\n",
    "        W_conv1 = tf.get_variable('W_conv1', shape=[5, 5, 1, n1])\n",
    "        b_conv1 = tf.get_variable('b_conv1', shape=[n1])\n",
    "        h_conv1 = tf.nn.relu(tf.add(conv(x_image, W_conv1), b_conv1))\n",
    "        # fc layer to logits\n",
    "        h_conv1_flat = tf.reshape(h_conv1, [-1, 28*28*n1])\n",
    "        W_fc1 = tf.get_variable('W_fc1', shape=[28*28*n1, 10])\n",
    "        b_fc1 = tf.get_variable('b_fc1', shape=[10])\n",
    "        logits = tf.add(tf.matmul(h_conv1_flat, W_fc1), b_fc1, name='logits_cnn1')\n",
    "    elif model_type=='cnn_cpcpff':\n",
    "        # 2 layer cnn, similar architecture to tensorflow's deep mnist tutorial, so you can compare\n",
    "        n1 = 32\n",
    "        n2 = 64\n",
    "        n3 = 1024\n",
    "        x_image = tf.reshape(x, [-1,28,28,1]) # batch, then width, height, channels\n",
    "        # cnn layer 1\n",
    "        W_conv1 = tf.get_variable('W_conv1', shape=[5, 5, 1, n1])\n",
    "        b_conv1 = tf.get_variable('b_conv1', shape=[n1])\n",
    "        h_conv1 = tf.nn.relu(tf.add(conv(x_image, W_conv1), b_conv1))\n",
    "        # pool 1\n",
    "        h_pool1 = maxpool(h_conv1)\n",
    "        # cnn layer 2\n",
    "        W_conv2 = tf.get_variable('W_conv2', shape=[5, 5, n1, n2])\n",
    "        b_conv2 = tf.get_variable('b_conv2', shape=[n2])\n",
    "        h_conv2 = tf.nn.relu(tf.add(conv(h_pool1, W_conv2), b_conv2))\n",
    "        # pool 2\n",
    "        h_pool2 = maxpool(h_conv2)\n",
    "        # fc layer to logits (7x7 since 2 rounds of maxpool)\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*n2])\n",
    "        W_fc1 = tf.get_variable('W_fc1', shape=[7*7*n2, n3])\n",
    "        b_fc1 = tf.get_variable('b_fc1', shape=[n3])\n",
    "        h_fc1 = tf.nn.relu(tf.add(tf.matmul(h_pool2_flat, W_fc1), b_fc1))\n",
    "        # one more fc layer\n",
    "        # ... again, this is the logistic layer with softmax readout\n",
    "        W_fc2 = tf.get_variable('W_fc2', shape=[n3,10])\n",
    "        b_fc2 = tf.get_variable('b_fc2', shape=[10])\n",
    "        logits = tf.add(tf.matmul(h_fc1, W_fc2), b_fc2, name='logits_cnn2')\n",
    "    elif model_type=='cnn_cpcpfdf':\n",
    "        # same as above but add dropout.\n",
    "        # 2 layer cnn, similar architecture to tensorflow's deep mnist tutorial, so you can compare\n",
    "        n1 = 32\n",
    "        n2 = 64\n",
    "        n3 = 1024\n",
    "        x_image = tf.reshape(x, [-1,28,28,1]) # batch, then width, height, channels\n",
    "        # cnn layer 1\n",
    "        W_conv1 = tf.get_variable('W_conv1', shape=[5, 5, 1, n1])\n",
    "        b_conv1 = tf.get_variable('b_conv1', shape=[n1])\n",
    "        h_conv1 = tf.nn.relu(tf.add(conv(x_image, W_conv1), b_conv1))\n",
    "        # pool 1\n",
    "        h_pool1 = maxpool(h_conv1)\n",
    "        # cnn layer 2\n",
    "        W_conv2 = tf.get_variable('W_conv2', shape=[5, 5, n1, n2])\n",
    "        b_conv2 = tf.get_variable('b_conv2', shape=[n2])\n",
    "        h_conv2 = tf.nn.relu(tf.add(conv(h_pool1, W_conv2), b_conv2))\n",
    "        # pool 2\n",
    "        h_pool2 = maxpool(h_conv2)\n",
    "        # fc layer to logits (7x7 since 2 rounds of maxpool)\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*n2])\n",
    "        W_fc1 = tf.get_variable('W_fc1', shape=[7*7*n2, n3])\n",
    "        b_fc1 = tf.get_variable('b_fc1', shape=[n3])\n",
    "        h_fc1 = tf.nn.relu(tf.add(tf.matmul(h_pool2_flat, W_fc1), b_fc1))\n",
    "        # insert a dropout layer here.\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, pkeep)\n",
    "        # one more fc layer\n",
    "        # ... again, this is the logistic layer with softmax readout\n",
    "        W_fc2 = tf.get_variable('W_fc2', shape=[n3,10])\n",
    "        b_fc2 = tf.get_variable('b_fc2', shape=[10])\n",
    "        logits = tf.add(tf.matmul(h_fc1_drop, W_fc2), b_fc2, name='logits_cnn2')\n",
    "    else: \n",
    "        print('error not a valid model type')\n",
    "\n",
    "    return logits\n",
    "\n",
    "def compute_cross_entropy(logits, y):\n",
    "    # Compute the average cross-entropy across all the examples.\n",
    "    numerical_instability_example = 1\n",
    "    if numerical_instability_example:\n",
    "        y_pred = tf.nn.softmax(logits, name='y_pred') # the predicted probability for each example.\n",
    "        cross_ent = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_pred), reduction_indices=[1]))\n",
    "    else:\n",
    "        sm_ce = tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=logits, name='cross_ent_terms')\n",
    "        cross_ent = tf.reduce_mean(sm_ce, name='cross_ent')\n",
    "    return cross_ent\n",
    "\n",
    "def compute_accuracy(logits, y):\n",
    "    prediction = tf.argmax(logits, 1, name='pred_class')\n",
    "    true_label = tf.argmax(y, 1, name='true_class')\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_label), tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choose case to run \n",
    "model_type = 'cnn_cf' \n",
    "dir_name = 'logs/scratch03_cnnz/{}'.format(model_type)\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After step   0, test accuracy 0.0793\n",
      "After step 100, test accuracy 0.3061\n",
      "After step 200, test accuracy 0.5411\n",
      "After step 300, test accuracy 0.6778\n",
      "After step 400, test accuracy 0.7334\n",
      "After step 500, test accuracy 0.7683\n",
      "After step 600, test accuracy 0.7911\n",
      "After step 700, test accuracy 0.8049\n",
      "After step 800, test accuracy 0.8180\n",
      "After step 900, test accuracy 0.8218\n",
      "After step 1000, test accuracy 0.8309\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    # We build the model here as before\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name='x')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='y')\n",
    "    pkeep = tf.placeholder(tf.float32, name='pkeep')\n",
    "    \n",
    "    with tf.name_scope('model'):\n",
    "        logits = compute_logits(x, model_type, pkeep)\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = compute_cross_entropy(logits=logits, y=y)\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = compute_accuracy(logits, y)\n",
    "    \n",
    "    with tf.name_scope('opt'):\n",
    "        if model_type=='lr':\n",
    "            opt = tf.train.GradientDescentOptimizer(0.5)\n",
    "        else:\n",
    "            opt = tf.train.AdamOptimizer(1e-4)\n",
    "        train_step = opt.minimize(loss)\n",
    "    \n",
    "    with tf.name_scope('summaries'):\n",
    "        # create summary for loss and accuracy\n",
    "        tf.summary.scalar('loss', loss) \n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        # create summary for logits\n",
    "        tf.summary.histogram('logits', logits)\n",
    "        # create summary for input image\n",
    "        tf.summary.image('input', tf.reshape(x, [-1, 28, 28, 1]))\n",
    "    \n",
    "        summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        summary_writer = tf.summary.FileWriter(dir_name, sess.graph)\n",
    "        #train_writer = tf.train.SummaryWriter(dir_name + '/train', sess.graph)\n",
    "        #test_writer = tf.train.SummaryWriter(dir_name + '/test')\n",
    "        summary_writer_train = tf.summary.FileWriter(dir_name+'/train', sess.graph)\n",
    "        summary_writer_test = tf.summary.FileWriter(dir_name+'/test')\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        for i in range(1001):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            X_batch = batch[0]\n",
    "            y_batch = batch[1]\n",
    "\n",
    "            # now run\n",
    "            _ , summary = sess.run((train_step, summary_op),\n",
    "                                      feed_dict={x: X_batch, y: y_batch, pkeep:0.5})\n",
    "            \n",
    "            # write the summary output to file\n",
    "            if i%10==0:\n",
    "                summary_writer_train.add_summary(summary, i)\n",
    "\n",
    "            # print diagnostics\n",
    "            #print(\".\", end='', flush=True)\n",
    "            #if i%100 == 0:\n",
    "            #    train_error = sess.run(accuracy, {x: mnist.train.images[0:1000,:], y: mnist.train.labels[0:1000,:]})\n",
    "            #    print(\"\\rAfter step {0:3d}, training accuracy {1:0.4f}\".format(i, train_error), flush=True)\n",
    "            if i%100 == 0: \n",
    "                (test_error, summary) = sess.run((accuracy,summary_op), {x:mnist.test.images, y:mnist.test.labels, pkeep:1.0})\n",
    "                print(\"\\rAfter step {0:3d}, test accuracy {1:0.4f}\".format(i, test_error), flush=True)\n",
    "                summary_writer_test.add_summary(summary, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
