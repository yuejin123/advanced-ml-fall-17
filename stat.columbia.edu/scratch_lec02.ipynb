{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced ML Part II // Lecture 02 Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# written by John P. Cunningham, for use in lecture\n",
    "# continues many of the conventions set out in Wenda Zhou's excellent tf tutorial\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate performance on some data \n",
    "def perf_eval(y_pred, y_true):\n",
    "    \"\"\"a function to evaluate performance of predicted y values vs true class labels\"\"\"\n",
    "    # now look at some data\n",
    "    print(' sample pred: {0}\\n sample true: {1}'.format(np.argmax(y_pred[0:20],1),np.argmax(y_true[0:20],1)))\n",
    "    # avg accuracy\n",
    "    is_correct_vals = np.equal(np.argmax(y_pred,1),np.argmax(y_true,1))\n",
    "    accuracy_vals = np.mean(is_correct_vals)\n",
    "    print(' mean classification accuracy: {0}%'.format(100*accuracy_vals))\n",
    "    # Dig in a little deeper.  Where did we make correct predictions?  Does this seem reasonable?\n",
    "    print(' correct predictions by class: {0}'.format(y_true[is_correct_vals,:].sum(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_save(x, fname='foo.png', extent=None, show=True, cmap='gray'):\n",
    "    plt.imshow(x,cmap=cmap,extent=extent)\n",
    "    plt.axis('off')\n",
    "    plt.savefig('tmp/'+fname,bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and sample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# get mnist data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('../data/mnist', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take a manageable dataset size for full gradient calculation\n",
    "X_train = mnist.train.images[0:5000,:]\n",
    "y_train = mnist.train.labels[0:5000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "some_digit = 1\n",
    "plot_save(X_train[some_digit,:].reshape(28,28), 'mnist_digit.png', show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a basic logistic regression model, as in Wenda's tf tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We start with our existing model code\n",
    "\n",
    "def compute_logits(x):\n",
    "    \"\"\"Compute the logits of the model\"\"\"\n",
    "    W = tf.get_variable('W', shape=[784, 10])\n",
    "    b = tf.get_variable('b', shape=[10])\n",
    "    \n",
    "    logits = tf.add(tf.matmul(x, W), b, name='logits')\n",
    "    return logits\n",
    "\n",
    "# Note: this function is implemented in tensorflow as\n",
    "# tf.nn.softmax_cross_entropy_with_logits\n",
    "\n",
    "# We have included it here for illustration only, please don't use it.\n",
    "def compute_cross_entropy(logits, y):\n",
    "    y_pred = tf.nn.softmax(logits, name='y_pred') # the predicted probability for each example.\n",
    "\n",
    "    # Compute the average cross-entropy across all the examples.\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_pred), axis=[1]))\n",
    "    return cross_entropy\n",
    "\n",
    "def compute_accuracy(logits, y):\n",
    "    prediction = tf.argmax(logits, 1, name='pred_class')\n",
    "    true_label = tf.argmax(y, 1, name='true_class')\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_label), tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choose case to run to demonstrate SGD; 0 corresponds to full grad\n",
    "batch_size = 100 \n",
    "if batch_size==0:\n",
    "    dir_name = 'logs/scratch02x/full_grad'\n",
    "else:\n",
    "    dir_name = 'logs/scratch02x/sgd{}'.format(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After step   0, training accuracy 0.2556\n",
      "After step   0, test accuracy 0.2374\n",
      "After step 100, training accuracy 0.8852............................................................\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    # We build the model here as before\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name='x')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='y')\n",
    "    \n",
    "    logits = compute_logits(x)\n",
    "    loss = compute_cross_entropy(logits=logits, y=y)\n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "    \n",
    "    opt = tf.train.GradientDescentOptimizer(0.5)\n",
    "    train_step = opt.minimize(loss)\n",
    "    \n",
    "    # create summary for loss and accuracy\n",
    "    tf.summary.scalar('loss', loss) \n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    # create summary for logits\n",
    "    tf.summary.histogram('logits', logits)\n",
    "    # create summary for input image\n",
    "    tf.summary.image('input', tf.reshape(x, [-1, 28, 28, 1]))\n",
    "    \n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        summary_writer = tf.summary.FileWriter(dir_name, sess.graph)\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        for i in range(101):\n",
    "            # choose batch\n",
    "            if batch_size==0:\n",
    "                X_batch = mnist.train.images #X_train \n",
    "                y_batch = mnist.train.labels #y_train\n",
    "            else:\n",
    "                batch = mnist.train.next_batch(batch_size)\n",
    "                X_batch = batch[0]\n",
    "                y_batch = batch[1]\n",
    "\n",
    "            # now run\n",
    "            _ , summary = sess.run((train_step, summary_op),\n",
    "                                      feed_dict={x: X_batch, y: y_batch})\n",
    "            \n",
    "            # write the summary output to file\n",
    "            summary_writer.add_summary(summary, i)\n",
    "\n",
    "            # print diagnostics\n",
    "            print(\".\", end='', flush=True)\n",
    "            if i%100 == 0:\n",
    "                train_error = sess.run(accuracy, {x: X_train, y: y_train})\n",
    "                print(\"\\rAfter step {0:3d}, training accuracy {1:0.4f}\".format(i, train_error), flush=True)\n",
    "            if i%1000 == 0: \n",
    "                test_error = sess.run(accuracy, {x:mnist.test.images, y:mnist.test.labels})\n",
    "                print(\"\\rAfter step {0:3d}, test accuracy {1:0.4f}\".format(i, test_error), flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple MSE example of SGD being unstable..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simpler set up to demonstrate risk of SGD\n",
    "# choose case to run to demonstrate SGD; 0 corresponds to full grad\n",
    "batch_size = 10\n",
    "decay_rate = 1\n",
    "if batch_size==0:\n",
    "    dir_name = 'logs/scratch02x_ex/full_grad'\n",
    "else:\n",
    "    dir_name = 'logs/scratch02x_ex/sgd{}_decay{}'.format(batch_size,decay_rate)\n",
    "        \n",
    "n = 21\n",
    "X = np.reshape(np.linspace(-10,10,n), [n,1])\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    w = tf.get_variable('w', shape=[1], initializer=tf.constant_initializer(-10))\n",
    "    x = tf.placeholder(tf.float32, [None,1], name='x')\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    mse = tf.reduce_mean(0.5*(x - w)**2)\n",
    "    theta = tf.reduce_mean(w)\n",
    "    \n",
    "    learning_rate = tf.train.inverse_time_decay(0.5, global_step, 1, decay_rate)\n",
    "    opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_step = opt.minimize(mse, global_step=global_step)\n",
    "    \n",
    "    tf.summary.scalar('mse', mse) \n",
    "    tf.summary.scalar('theta', theta ) \n",
    "    tf.summary.histogram('foo', x)\n",
    "    \n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        summary_writer = tf.summary.FileWriter(dir_name, sess.graph)\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(101):\n",
    "            if batch_size==0:\n",
    "                X_iter = X\n",
    "            else:\n",
    "                batch = np.floor(np.random.rand(batch_size)*n).astype(int)\n",
    "                X_iter = X[batch]\n",
    "                \n",
    "            #_, lo = sess.run((train_step,mse), feed_dict={x: X_iter})\n",
    "            _, lo, summary = sess.run((train_step, mse, summary_op), feed_dict={x: X_iter})\n",
    "            #print(lo)\n",
    "            #print(sess.run(learning_rate))\n",
    "            # write the summary output to file\n",
    "            summary_writer.add_summary(summary, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
